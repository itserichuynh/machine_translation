{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39ba4f8b-5a60-418e-a3fe-cb6fff89d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "495b6569-2167-48f9-b7c2-8097dd48e1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"SOS\": 0, \"EOS\": 1, \"unk\": 2}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"unk\"}\n",
    "        self.n_words = 3  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def trim_vocab(self, min_occurance):\n",
    "        words_to_delete = [word for word, count in self.word2count.items() if count < min_occurance]\n",
    "\n",
    "        if \"unk\" not in self.word2count:\n",
    "            self.word2count[\"unk\"] = 0\n",
    "\n",
    "        for word in words_to_delete:\n",
    "            self.word2count[\"unk\"] += self.word2count[word]\n",
    "            del self.word2index[word]\n",
    "            del self.word2count[word]\n",
    "\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"unk\"}\n",
    "        self.n_words = 3  # Count SOS and EOS\n",
    "\n",
    "        for word in self.word2count.keys():\n",
    "            if word != 'unk':\n",
    "                self.word2index[word] = self.n_words\n",
    "                self.index2word[self.n_words] = word\n",
    "                self.n_words += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6485e08a-e68b-47ee-84c7-6f21d4095b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36e81dcb-2772-446c-a5c0-801f25209986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/tatoeba/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48d19fb7-6148-4363-aa35-2b2ab8d982d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there are a lot of example sentences and we want to train something quickly,\n",
    "# we'll trim the data set to only relatively short and simple sentences.\n",
    "# Here the maximum length is 10 words (that includes ending punctuation) and \n",
    "# we're filtering to sentences that translate to the form \"I am\" or \"He is\" etc. (accounting for apostrophes replaced earlier).\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = [\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \", \"I don t\", \"Do you\", \"I want\", \"Are you\", \"I have\", \"I think\",\n",
    "       \"I can t\", \"I was\", \"He is\", \"I m not\", \"This is\", \"I just\", \"I didn t\",\n",
    "       \"I am\", \"I thought\", \"I know\", \"Tom is\", \"I had\", \"Did you\", \"Have you\",\n",
    "       \"Can you\", \"He was\", \"You don t\", \"I d like\", \"It was\", \"You should\",\n",
    "       \"Would you\", \"I like\", \"It is\", \"She is\", \"You can t\", \"He has\",\n",
    "       \"What do\", \"If you\", \"I need\", \"No one\", \"You are\", \"You have\",\n",
    "       \"I feel\", \"I really\", \"Why don t\", \"I hope\", \"I will\", \"We have\",\n",
    "       \"You re not\", \"You re very\", \"She was\", \"I love\", \"You must\", \"I can\"]\n",
    "eng_prefixes = (map(lambda x: x.lower(), eng_prefixes))\n",
    "eng_prefixes = tuple(eng_prefixes)\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f97c8882-8ad8-44b8-81af-48ea5963815b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 37298 sentence pairs\n",
      "Counting words...\n",
      "Mark all OOV with 'unk' for all lines\n",
      "Counted words:\n",
      "fra 5314\n",
      "eng 3755\n"
     ]
    }
   ],
   "source": [
    "# Read text file and split into lines, split lines into pairs\n",
    "# Normalize text, filter by length and content\n",
    "# Make word lists from sentences in pairs\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "\n",
    "    # trim down the data according to specification above\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "\n",
    "    # remove all words with a frequency below a threshold\n",
    "    print(\"Mark all OOV with 'unk' for all lines\")\n",
    "    input_lang.trim_vocab(2)\n",
    "    output_lang.trim_vocab(2)\n",
    "\n",
    "    # update pairs with 'unk'\n",
    "    new_pairs = []\n",
    "    for pair in pairs:\n",
    "        new_pair = []\n",
    "        input_tokens = []\n",
    "        output_tokens = []\n",
    "        for word in pair[0].split(' '):\n",
    "            if word in input_lang.word2index:\n",
    "                input_tokens.append(word)\n",
    "            else:\n",
    "                input_tokens.append('unk')\n",
    "\n",
    "        for word in pair[1].split(' '):\n",
    "            if word in output_lang.word2index:\n",
    "                output_tokens.append(word)\n",
    "            else:\n",
    "                output_tokens.append('unk')\n",
    "\n",
    "        new_pair.append(\" \".join(input_tokens))\n",
    "        new_pair.append(\" \".join(output_tokens))\n",
    "        new_pairs.append(new_pair)\n",
    "\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "\n",
    "    return input_lang, output_lang, new_pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e6a64fd-2970-47e2-8ff1-92d013726ccf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15bb3474-ead7-4181-84ae-0b5172aea261",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded =  self.dropout(self.embedding(input))\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "651df94d-bd4c-40cf-ac09-8a9e17e35428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "\n",
    "    n = len(pairs)\n",
    "    n_test = int(0.1 * n)\n",
    "    n_dev = int(0.2 * n)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    random.shuffle(pairs)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                               torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "    train_data = torch.utils.data.Subset(data, range(n_dev, n))\n",
    "    dev_data = torch.utils.data.Subset(data, range(n_test, n_dev))\n",
    "    test_data = torch.utils.data.Subset(data, range(n_test))\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    dev_sampler = RandomSampler(dev_data)\n",
    "    dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=batch_size)\n",
    "\n",
    "    test_sampler = RandomSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "    return input_lang, output_lang, train_dataloader, dev_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6370b96-af2f-4ae1-b858-f584ee542e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion):\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db543138-abee-42aa-b632-7f9a4265650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc3b635d-12af-441c-834f-d598977f86b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, val_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "               print_every=100, plot_every=100):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    vloss_avg = 0\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        encoder.train(True)\n",
    "        decoder.train(True)\n",
    "        train_loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += train_loss\n",
    "        plot_loss_total += train_loss\n",
    "\n",
    "        # calculate loss in dev set\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "\n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "        running_vloss = 0\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(val_dataloader):\n",
    "                input_tensor, target_tensor = vdata\n",
    "                encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "                decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "                val_loss = criterion(\n",
    "                    decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "                    target_tensor.view(-1)\n",
    "                )\n",
    "                running_vloss += val_loss\n",
    "                \n",
    "        vloss_avg += running_vloss / (i + 1)\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "\n",
    "            print_vloss_avg = vloss_avg / print_every\n",
    "            vloss_avg = 0\n",
    "            print('%s (%d %d%%) %.4f %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_loss_avg, print_vloss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e7cb1e6-eb30-4eba-a5f6-627f5afe7a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "baeb0e44-2d51-4778-9f6b-49a678b92a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 37298 sentence pairs\n",
      "Counting words...\n",
      "Mark all OOV with 'unk' for all lines\n",
      "Counted words:\n",
      "fra 5314\n",
      "eng 3755\n",
      "2m 17s (- 34m 22s) (5 6%) 1.3996 1.2843\n",
      "4m 37s (- 32m 19s) (10 12%) 0.5692 0.8367\n",
      "6m 57s (- 30m 8s) (15 18%) 0.3421 0.7534\n",
      "9m 16s (- 27m 50s) (20 25%) 0.2426 0.7482\n",
      "11m 35s (- 25m 29s) (25 31%) 0.1878 0.7622\n",
      "13m 52s (- 23m 7s) (30 37%) 0.1542 0.7814\n",
      "16m 11s (- 20m 48s) (35 43%) 0.1319 0.8022\n",
      "18m 30s (- 18m 30s) (40 50%) 0.1165 0.8263\n",
      "20m 48s (- 16m 10s) (45 56%) 0.1047 0.8469\n",
      "23m 6s (- 13m 51s) (50 62%) 0.0963 0.8718\n",
      "25m 24s (- 11m 32s) (55 68%) 0.0890 0.8919\n",
      "27m 43s (- 9m 14s) (60 75%) 0.0839 0.9150\n",
      "30m 1s (- 6m 55s) (65 81%) 0.0789 0.9261\n",
      "32m 20s (- 4m 37s) (70 87%) 0.0746 0.9419\n",
      "34m 38s (- 2m 18s) (75 93%) 0.0719 0.9575\n",
      "36m 56s (- 0m 0s) (80 100%) 0.0692 0.9735\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader, val_dataloader, test_dataloader = get_dataloader(batch_size)\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "train(train_dataloader, val_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b06a202e-196d-4718-bc62-066f8f187828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_test_set(test_dataloader, encoder, decoder, criterion):\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader:\n",
    "            input_tensor, target_tensor = data\n",
    "    \n",
    "            encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "            decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "    \n",
    "            loss = criterion(\n",
    "                decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "                target_tensor.view(-1)\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(test_dataloader)\n",
    "    \n",
    "test_loss = evaluate_on_test_set(test_dataloader, encoder, decoder, nn.NLLLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1469a88b-70ab-42fb-9bc4-60a55f528ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9593960790552645"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e87c7729-2c78-4782-9e70-b6e9b05e55f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "    return decoded_words, decoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "154ce52f-8b59-4877-913e-d90e7f719ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d2c7f316-7e8e-4952-99d7-ffcb75c786f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> je ne vais pas encore abandonner\n",
      "= i m not giving up yet\n",
      "< i m not giving up yet <EOS>\n",
      "\n",
      "> que pensez vous de mon costume ?\n",
      "= what do you think of my costume ?\n",
      "< what do you think of my costume ? <EOS>\n",
      "\n",
      "> elle etait clouee au lit hier\n",
      "= she was sick in bed yesterday\n",
      "< she was sick in bed for tomorrow night <EOS>\n",
      "\n",
      "> il fut entendu en train de chanter la chanson\n",
      "= he was heard singing the song\n",
      "< he was heard singing the song <EOS>\n",
      "\n",
      "> vous interessez vous aux langues etrangeres ?\n",
      "= are you interested in foreign languages ?\n",
      "< are you interested in foreign languages ? <EOS>\n",
      "\n",
      "> nous en sommes toutes responsables\n",
      "= we re all to blame for that\n",
      "< we re all to blame for that <EOS>\n",
      "\n",
      "> je veux chanter une chanson\n",
      "= i want to sing a song\n",
      "< i want to sing a song a song <EOS>\n",
      "\n",
      "> j espere qu il va reussir\n",
      "= i hope that he will succeed\n",
      "< i hope he will succeed <EOS>\n",
      "\n",
      "> j aimerais que tu rencontres ma femme\n",
      "= i d like you to meet my wife\n",
      "< i d like you to meet my wife <EOS>\n",
      "\n",
      "> il a les yeux unk\n",
      "= he has brown eyes\n",
      "< he has brown eyes before you <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1fe85bef-1015-4987-b7c6-b205223cf662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i have the study in the trouble <EOS>'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "output_words, _ = evaluate(encoder, decoder, \"le sous les\", input_lang, output_lang)\n",
    "output_sentence = ' '.join(output_words)\n",
    "output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0052e55f-aae4-4a0a-8e32-a0c79fa85b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'je n en suis pas certain'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair = random.choice(pairs)\n",
    "pair[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5f55d7-2639-49d6-a334-f257567b342a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
