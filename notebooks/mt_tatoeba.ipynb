{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39ba4f8b-5a60-418e-a3fe-cb6fff89d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "495b6569-2167-48f9-b7c2-8097dd48e1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"unk\"}\n",
    "        self.n_words = 3  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def trim_vocab(self, min_occurance):\n",
    "        if \"unk\" not in self.word2count:\n",
    "            self.word2count[\"unk\"] = 0\n",
    "\n",
    "        words_to_delete = [word for word, count in self.word2count.items() if count < min_occurance and word != \"unk\"]\n",
    "\n",
    "        for word in words_to_delete:\n",
    "            self.word2count[\"unk\"] += self.word2count[word]\n",
    "            del self.word2index[word]\n",
    "            del self.word2count[word]\n",
    "\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"unk\"}\n",
    "        self.n_words = 3  # Count SOS and EOS\n",
    "\n",
    "        for word in self.word2count.keys():\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6485e08a-e68b-47ee-84c7-6f21d4095b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36e81dcb-2772-446c-a5c0-801f25209986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('data/tatoeba/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48d19fb7-6148-4363-aa35-2b2ab8d982d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there are a lot of example sentences and we want to train something quickly,\n",
    "# we'll trim the data set to only relatively short and simple sentences.\n",
    "# Here the maximum length is 10 words (that includes ending punctuation) and \n",
    "# we're filtering to sentences that translate to the form \"I am\" or \"He is\" etc. (accounting for apostrophes replaced earlier).\n",
    "MAX_LENGTH = 12\n",
    "\n",
    "eng_prefixes = [\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \", \"I don t\", \"Do you\", \"I want\", \"Are you\", \"I have\", \"I think\",\n",
    "       \"I can t\", \"I was\", \"He is\", \"I m not\", \"This is\", \"I just\", \"I didn t\",\n",
    "       \"I am\", \"I thought\", \"I know\", \"Tom is\", \"I had\", \"Did you\", \"Have you\",\n",
    "       \"Can you\", \"He was\", \"You don t\", \"I d like\", \"It was\", \"You should\",\n",
    "       \"Would you\", \"I like\", \"It is\", \"She is\", \"You can t\", \"He has\",\n",
    "       \"What do\", \"If you\", \"I need\", \"No one\", \"You are\", \"You have\",\n",
    "       \"I feel\", \"I really\", \"Why don t\", \"I hope\", \"I will\", \"We have\",\n",
    "       \"You re not\", \"You re very\", \"She was\", \"I love\", \"You must\", \"I can\"]\n",
    "eng_prefixes = (map(lambda x: x.lower(), eng_prefixes))\n",
    "eng_prefixes = tuple(eng_prefixes)\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f97c8882-8ad8-44b8-81af-48ea5963815b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 43152 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "fra 9501\n",
      "eng 5993\n",
      "Mark all OOV with 'unk' for all lines\n",
      "We have 43152 pairs of sentence without unk\n"
     ]
    }
   ],
   "source": [
    "# Read text file and split into lines, split lines into pairs\n",
    "# Normalize text, filter by length and content\n",
    "# Make word lists from sentences in pairs\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "\n",
    "    # trim down the data according to specification above\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "\n",
    "    # remove all words with a frequency below a threshold\n",
    "    print(\"Mark all OOV with 'unk' for all lines\")\n",
    "    input_lang.trim_vocab(2)\n",
    "    output_lang.trim_vocab(2)\n",
    "\n",
    "    # update pairs with 'unk'\n",
    "    new_pairs = []\n",
    "    for pair in pairs:\n",
    "        new_pair = []\n",
    "        input_tokens = []\n",
    "        output_tokens = []\n",
    "        for word in pair[0].split(' '):\n",
    "            if word in input_lang.word2index:\n",
    "                input_tokens.append(word)\n",
    "            else:\n",
    "                input_tokens.append('unk')\n",
    "\n",
    "        for word in pair[1].split(' '):\n",
    "            if word in output_lang.word2index:\n",
    "                output_tokens.append(word)\n",
    "            else:\n",
    "                output_tokens.append('unk')\n",
    "\n",
    "        new_pair.append(\" \".join(input_tokens))\n",
    "        new_pair.append(\" \".join(output_tokens))\n",
    "        new_pairs.append(new_pair)\n",
    "\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "\n",
    "    return input_lang, output_lang, new_pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5e6a64fd-2970-47e2-8ff1-92d013726ccf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_lang\u001b[38;5;241m.\u001b[39mword2count[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "input_lang.word2count['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "15bb3474-ead7-4181-84ae-0b5172aea261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'touchee'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(input_lang.word2count, key=input_lang.word2count.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651df94d-bd4c-40cf-ac09-8a9e17e35428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
