{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "172eeadc-b24d-4a95-9118-b356b13ad0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39ba4f8b-5a60-418e-a3fe-cb6fff89d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2e4edcc-13b1-493f-b6aa-5fb87035ea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "import datasets\n",
    "import torchtext\n",
    "import tqdm\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f0f0b74-2d80-436d-923b-4018667e5f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16154ead-44d4-40bc-945c-effd6e5366d2",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6485e08a-e68b-47ee-84c7-6f21d4095b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36e81dcb-2772-446c-a5c0-801f25209986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(lang1, lang2):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('../data/tatoeba/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    # if reverse:\n",
    "    #     pairs = [list(reversed(p)) for p in pairs]\n",
    "    #     input_lang = Lang(lang2)\n",
    "    #     output_lang = Lang(lang1)\n",
    "    # else:\n",
    "    #     input_lang = Lang(lang1)\n",
    "    #     output_lang = Lang(lang2)\n",
    "\n",
    "    data = [{lang1: pair[0], lang2: pair[1]}for pair in pairs]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7f7890a-2ff6-4c2a-860a-adbbd2898325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n"
     ]
    }
   ],
   "source": [
    "data = readData('en', 'fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06e7c943-7478-4919-a8a3-6d34eeddf77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'en': 'go', 'fr': 'va !'},\n",
       " {'en': 'run !', 'fr': 'cours !'},\n",
       " {'en': 'run !', 'fr': 'courez !'},\n",
       " {'en': 'wow !', 'fr': 'ca alors !'},\n",
       " {'en': 'fire !', 'fr': 'au feu !'},\n",
       " {'en': 'help !', 'fr': 'a l aide !'},\n",
       " {'en': 'jump', 'fr': 'saute'},\n",
       " {'en': 'stop !', 'fr': 'ca suffit !'},\n",
       " {'en': 'stop !', 'fr': 'stop !'},\n",
       " {'en': 'stop !', 'fr': 'arrete toi !'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34904996-cf47-4425-b200-aa0b3f1ec42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'en': 'it s perfect', 'fr': 'c est parfait'},\n",
       " {'en': 'i was sitting next to a man who clearly had a lot on his mind',\n",
       "  'fr': 'j etais assise a cote d un homme qui avait a l evidence de nombreuses choses en tete'},\n",
       " {'en': 'we have a gig at the club tomorrow night',\n",
       "  'fr': 'nous faisons un b uf au club demain soir'},\n",
       " {'en': 'i m not going to hurt you', 'fr': 'je ne vais pas te faire de mal'},\n",
       " {'en': 'apart from a filing tray full of papers on the desk everything else in the room was on the floor',\n",
       "  'fr': 'en dehors d une classeur plein de papiers sur le bureau tout le reste dans la piece etait par terre'},\n",
       " {'en': 'i d like to visit your country someday',\n",
       "  'fr': 'j aimerais visiter ton pays un de ces jours'},\n",
       " {'en': 'how thoughtful of you to have chilled some wine for us',\n",
       "  'fr': 'comme c est attentionne de votre part d avoir mis au frais du vin pour nous'},\n",
       " {'en': 'you should cut up your meat before you eat it',\n",
       "  'fr': 'tu devrais couper ta viande en morceaux avant de la manger'},\n",
       " {'en': 'an athlete must keep in good condition',\n",
       "  'fr': 'un athlete doit rester en forme'},\n",
       " {'en': 'you need to be more patient',\n",
       "  'fr': 'il vous faut etre plus patiente'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "# print(\"Before shuffle:\", data[:10])\n",
    "random.shuffle(data)\n",
    "# print(\"After shuffle:\", data[:10])\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fc12cc8-497c-423d-8983-80f876e9a06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splittng data\n",
    "n = len(data)\n",
    "n_train = int(0.8*n)\n",
    "n_val = int(0.9*n)\n",
    "\n",
    "train_data = data[:n_train]\n",
    "val_data = data[n_train:n_val]\n",
    "test_data = data[n_val:]\n",
    "\n",
    "# convert to Huggingface dataset\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "# Create the DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9979ef23-6b66-4691-9bf0-ec40bb997cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'fr'],\n",
       "        num_rows: 108673\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['en', 'fr'],\n",
       "        num_rows: 13584\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'fr'],\n",
       "        num_rows: 13585\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "936c65b7-63ab-4553-9e84-d5f636a85a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135842"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1935ebe-e295-42c6-bee2-9602e920b81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'it s perfect', 'fr': 'c est parfait'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daf9b47d-9f45-40e7-96fb-fcb622490034",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "fr_nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d288c24-9e5d-41bc-8441-395eedd33361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', 'a', 'lovely', 'day', 'it', 'is', 'today', '!']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"What a lovely day it is today!\"\n",
    "\n",
    "[token.text for token in en_nlp.tokenizer(string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05ae8750-8638-4c08-b352-4877982e4ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_example(example, en_nlp, fr_nlp, max_length, lower, sos_token, eos_token):\n",
    "    en_tokens = [token.text for token in en_nlp.tokenizer(example[\"en\"])][:max_length]\n",
    "    fr_tokens = [token.text for token in fr_nlp.tokenizer(example[\"fr\"])][:max_length]\n",
    "    if lower:\n",
    "        en_tokens = [token.lower() for token in en_tokens]\n",
    "        fr_tokens = [token.lower() for token in fr_tokens]\n",
    "    en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "    fr_tokens = [sos_token] + fr_tokens + [eos_token]\n",
    "    return {\"en_tokens\": en_tokens, \"fr_tokens\": fr_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef42e644-a5d6-4e81-abe9-a1876b552a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2094d7f88f56401cb72a2945fcce9fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/108673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58d730b372c4627922beca1626780db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13584 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb5e624c72840a2b823a6f68b871a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13585 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = 1_000\n",
    "lower = True\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "fn_kwargs = {\n",
    "    \"en_nlp\": en_nlp,\n",
    "    \"fr_nlp\": fr_nlp,\n",
    "    \"max_length\": max_length,\n",
    "    \"lower\": lower,\n",
    "    \"sos_token\": sos_token,\n",
    "    \"eos_token\": eos_token,\n",
    "}\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "val_dataset = val_dataset.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "test_dataset = test_dataset.map(tokenize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ee1ab18-4420-4309-b4aa-1bf9bb494e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'it s perfect',\n",
       " 'fr': 'c est parfait',\n",
       " 'en_tokens': ['<sos>', 'it', 's', 'perfect', '<eos>'],\n",
       " 'fr_tokens': ['<sos>', 'c', 'est', 'parfait', '<eos>']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b83189d-35d5-445c-8765-e16036f87535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "min_freq = 2\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "special_tokens = [\n",
    "    unk_token,\n",
    "    pad_token,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "]\n",
    "\n",
    "en_vocab = build_vocab_from_iterator(\n",
    "    train_dataset[\"en_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")\n",
    "\n",
    "fr_vocab = build_vocab_from_iterator(\n",
    "    train_dataset[\"fr_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad9fd9ed-837c-4ed0-aeb6-2550a803f0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '<sos>', '<eos>', 'i', 'you', 'to', 'the', '?', 'a']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1ee3a37-4998-48c4-a8af-133a030e8195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '<sos>', '<eos>', 'je', 'a', 'de', '?', 'pas', 'vous']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_vocab.get_itos()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b28c4e21-22be-42fc-b36e-3c3fa5921776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5f14985-75d8-4c66-955f-3ade284de109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8135, 12253)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_vocab), len(fr_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d82ebb20-1056-41ce-8ea2-2338e59dce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert en_vocab[unk_token] == fr_vocab[unk_token]\n",
    "assert en_vocab[pad_token] == fr_vocab[pad_token]\n",
    "\n",
    "unk_index = en_vocab[unk_token]\n",
    "pad_index = en_vocab[pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7e9a13f-be4c-478d-8bd4-edcbfb43b94d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Token The not found and default index is not set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m en_vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/torchtext/vocab/vocab.py:65\u001b[0m, in \u001b[0;36mVocab.__getitem__\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mexport\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, token: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m        token: The token used to lookup the corresponding index.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m        The index corresponding to the associated token.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab[token]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Token The not found and default index is not set"
     ]
    }
   ],
   "source": [
    "en_vocab['The']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0672db85-316b-49cc-9965-f521cf9e2774",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab.set_default_index(unk_index)\n",
    "fr_vocab.set_default_index(unk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62037a77-2878-463d-ac24-2f9bbb808220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab['The']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78749614-4afe-4d39-a200-98f2b88735fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab.get_itos()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6fc597e-9efe-456a-9818-a19c0feab279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 136, 539, 1060, 3174]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [\"i\", \"love\", \"watching\", \"crime\", \"shows\"]\n",
    "en_vocab.lookup_indices(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7efd029a-1822-4b51-be70-68a153625bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'watching', 'crime', 'shows']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab.lookup_tokens(en_vocab.lookup_indices(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b4051b3-523e-4e0d-b3e3-778480a1fb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_example(example, en_vocab, fr_vocab):\n",
    "    en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])\n",
    "    fr_ids = fr_vocab.lookup_indices(example[\"fr_tokens\"])\n",
    "    return {\"en_ids\": en_ids, \"fr_ids\": fr_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2259c4c9-c1a1-47d4-a8f3-43000c1d5edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997f345d22da4be889c512146eebeb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/108673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37101620f2ab4d6886e0d3337f1a8213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13584 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3cd57daef74891966a14bd559862a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13585 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fn_kwargs = {\"en_vocab\": en_vocab, \"fr_vocab\": fr_vocab}\n",
    "\n",
    "train_dataset = train_dataset.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "val_dataset = val_dataset.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "test_dataset = test_dataset.map(numericalize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0126392-63ea-48b4-a44c-b4460e95fb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'it s perfect',\n",
       " 'fr': 'c est parfait',\n",
       " 'en_tokens': ['<sos>', 'it', 's', 'perfect', '<eos>'],\n",
       " 'fr_tokens': ['<sos>', 'c', 'est', 'parfait', '<eos>'],\n",
       " 'en_ids': [2, 12, 15, 749, 3],\n",
       " 'fr_ids': [2, 33, 10, 1072, 3]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5472f0fa-ca58-49df-8225-a95feb78a872",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"torch\"\n",
    "format_columns = [\"en_ids\", \"fr_ids\"]\n",
    "\n",
    "train_dataset = train_dataset.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")\n",
    "\n",
    "test_dataset = test_dataset.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8a412cf-ae03-4b5e-8f91-5b19a5d82318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en_ids': tensor([  2,  12,  15, 749,   3]),\n",
       " 'fr_ids': tensor([   2,   33,   10, 1072,    3]),\n",
       " 'en': 'it s perfect',\n",
       " 'fr': 'c est parfait',\n",
       " 'en_tokens': ['<sos>', 'it', 's', 'perfect', '<eos>'],\n",
       " 'fr_tokens': ['<sos>', 'c', 'est', 'parfait', '<eos>']}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43dcda7f-23d9-402b-b3a3-482b5cf0a6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en_ids': tensor([  2,  12,  15, 749,   3]),\n",
       " 'fr_ids': tensor([   2,   33,   10, 1072,    3]),\n",
       " 'en': 'it s perfect',\n",
       " 'fr': 'c est parfait',\n",
       " 'en_tokens': ['<sos>', 'it', 's', 'perfect', '<eos>'],\n",
       " 'fr_tokens': ['<sos>', 'c', 'est', 'parfait', '<eos>']}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd1a6a4-9c12-4ef7-9479-fe8917f11ed4",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11a0dc84-4302-4500-9c74-78e26189e5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_en_ids = [example[\"en_ids\"] for example in batch]\n",
    "        batch_fr_ids = [example[\"fr_ids\"] for example in batch]\n",
    "        en_lens = [example[\"en_ids\"].shape[0] for example in batch]\n",
    "        fr_lens = [example[\"fr_ids\"].shape[0] for example in batch]\n",
    "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n",
    "        batch_fr_ids = nn.utils.rnn.pad_sequence(batch_fr_ids, padding_value=pad_index)\n",
    "        batch = {\n",
    "            \"en_ids\": batch_en_ids,\n",
    "            \"fr_ids\": batch_fr_ids,\n",
    "            \"en_lens\": en_lens,\n",
    "            \"fr_lens\": fr_lens\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba61c82b-6019-428a-92b6-23e775d323e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d37c284b-467d-4963-8535-8848023f4eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_data_loader = get_data_loader(train_dataset, batch_size, pad_index, shuffle=True)\n",
    "valid_data_loader = get_data_loader(val_dataset, batch_size, pad_index)\n",
    "test_data_loader = get_data_loader(test_dataset, batch_size, pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7382b22-5517-4dd8-96e5-b613d2736f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "15132e3e-6ec4-4ee3-98da-380436ecee87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1cb4f4cb-2793-4f88-b6ae-cdaf2b985326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "850"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bafc0aa-8ed6-4e48-bb74-c67c1c3b8f48",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2cd78b8-6e7a-4a03-b50f-db99b4b020d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_lens):\n",
    "        # src = [src length, batch size]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [src length, batch size, embedding dim]\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, np.array(src_lens), enforce_sorted=False, batch_first=False)\n",
    "        packed_outputs, (hidden, cell) = self.rnn(packed)\n",
    "        # outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # outputs = [src length, batch size, hidden dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # outputs are always from the top hidden layer\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ec6bd65-52ad-4248-8d79-1437d807b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input = [batch size]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # n directions in the decoder will both always be 1, therefore:\n",
    "        # hidden = [n layers, batch size, hidden dim]\n",
    "        # context = [n layers, batch size, hidden dim]\n",
    "        input = input.unsqueeze(0)\n",
    "        # input = [1, batch size]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [1, batch size, embedding dim]\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        # output = [seq length, batch size, hidden dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # seq length and n directions will always be 1 in this decoder, therefore:\n",
    "        # output = [1, batch size, hidden dim]\n",
    "        # hidden = [n layers, batch size, hidden dim]\n",
    "        # cell = [n layers, batch size, hidden dim]\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        # prediction = [batch size, output dim]\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aadbceed-85fc-4101-925a-b8683f6ed8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        assert (\n",
    "            encoder.hidden_dim == decoder.hidden_dim\n",
    "        ), \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert (\n",
    "            encoder.n_layers == decoder.n_layers\n",
    "        ), \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, src, src_lens, trg, trg_lens, teacher_forcing_ratio):\n",
    "        # src = [src length, batch size]\n",
    "        # trg = [trg length, batch size]\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\n",
    "        # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_length = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)\n",
    "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src, src_lens)\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[0, :]\n",
    "        # input = [batch size]\n",
    "        for t in range(1, trg_length):\n",
    "            # insert input token embedding, previous hidden and previous cell states\n",
    "            # receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            # output = [batch size, output dim]\n",
    "            # hidden = [n layers, batch size, hidden dim]\n",
    "            # cell = [n layers, batch size, hidden dim]\n",
    "            # place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            # decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            # get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1)\n",
    "            # if teacher forcing, use actual next token as next input\n",
    "            # if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "            # input = [batch size]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2689f030-ac73-4f1e-b234-7c08a04c6a13",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b37c761b-ae7e-4e8c-b674-972407363db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(fr_vocab)\n",
    "output_dim = len(en_vocab)\n",
    "encoder_embedding_dim = 256\n",
    "decoder_embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_dim,\n",
    "    encoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    encoder_dropout,\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    output_dim,\n",
    "    decoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    decoder_dropout,\n",
    ")\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aad77bee-231e-4a58-aff0-2e2dc55f40f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(12253, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(8135, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=8135, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c26f1d8-c966-464c-8341-ea335be4c01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 16,748,999 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "97c50989-40e5-4d91-be13-d2449961d6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)\n",
    "# scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a6977cfc-e8b5-4394-9e3a-68e3465a4eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device\n",
    "):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        src = batch[\"fr_ids\"].to(device)\n",
    "        src_lens = batch[\"fr_lens\"]\n",
    "        trg = batch[\"en_ids\"].to(device)\n",
    "        trg_lens = batch[\"en_lens\"]\n",
    "        # src = [src length, batch size]\n",
    "        # trg = [trg length, batch size]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, src_lens, trg, trg_lens, teacher_forcing_ratio)\n",
    "        # output = [trg length, batch size, trg vocab size]\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        # output = [(trg length - 1) * batch size, trg vocab size]\n",
    "        trg = trg[1:].view(-1)\n",
    "        # trg = [(trg length - 1) * batch size]\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e475dff0-21fb-489a-91ff-e81aba19719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fn(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            src = batch[\"fr_ids\"].to(device)\n",
    "            src_lens = batch[\"fr_lens\"]\n",
    "            trg = batch[\"en_ids\"].to(device)\n",
    "            trg_lens = batch[\"en_lens\"]\n",
    "            # src = [src length, batch size]\n",
    "            # trg = [trg length, batch size]\n",
    "            output = model(src, src_lens, trg, trg_lens, 0)  # turn off teacher forcing\n",
    "            # output = [trg length, batch size, trg vocab size]\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            # output = [(trg length - 1) * batch size, trg vocab size]\n",
    "            trg = trg[1:].view(-1)\n",
    "            # trg = [(trg length - 1) * batch size]\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7c0aa914-961b-433f-814f-b9970fc12764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '../models/tatoeba/2024-11-30' already exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▍                                                                | 1/20 [02:51<54:23, 171.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.426 | Train PPL:  83.585\n",
      "\tValid Loss:   3.987 | Valid PPL:  53.870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████▊                                                             | 2/20 [06:13<56:53, 189.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.400 | Train PPL:  29.951\n",
      "\tValid Loss:   3.394 | Valid PPL:  29.791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████████▏                                                         | 3/20 [10:07<59:23, 209.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.859 | Train PPL:  17.445\n",
      "\tValid Loss:   3.016 | Valid PPL:  20.400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████▏                                                    | 4/20 [14:38<1:02:21, 233.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.479 | Train PPL:  11.928\n",
      "\tValid Loss:   2.786 | Valid PPL:  16.216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|████████████████▌                                                 | 5/20 [19:32<1:03:54, 255.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.216 | Train PPL:   9.171\n",
      "\tValid Loss:   2.613 | Valid PPL:  13.636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████▊                                              | 6/20 [24:49<1:04:30, 276.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.013 | Train PPL:   7.488\n",
      "\tValid Loss:   2.494 | Valid PPL:  12.105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███████████████████████                                           | 7/20 [31:21<1:08:05, 314.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.848 | Train PPL:   6.344\n",
      "\tValid Loss:   2.417 | Valid PPL:  11.215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████▍                                       | 8/20 [38:02<1:08:21, 341.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.729 | Train PPL:   5.633\n",
      "\tValid Loss:   2.365 | Valid PPL:  10.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|█████████████████████████████▋                                    | 9/20 [45:03<1:07:12, 366.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.630 | Train PPL:   5.104\n",
      "\tValid Loss:   2.320 | Valid PPL:  10.174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████▌                                | 10/20 [52:14<1:04:23, 386.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.537 | Train PPL:   4.650\n",
      "\tValid Loss:   2.282 | Valid PPL:   9.799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|███████████████████████████████████▊                             | 11/20 [59:42<1:00:47, 405.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.469 | Train PPL:   4.346\n",
      "\tValid Loss:   2.273 | Valid PPL:   9.711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████                          | 12/20 [1:07:26<56:24, 423.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.412 | Train PPL:   4.102\n",
      "\tValid Loss:   2.233 | Valid PPL:   9.324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████████████████████████████████████████▎                      | 13/20 [1:16:03<52:42, 451.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.354 | Train PPL:   3.873\n",
      "\tValid Loss:   2.233 | Valid PPL:   9.325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████▌                   | 14/20 [1:26:21<50:12, 502.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.311 | Train PPL:   3.710\n",
      "\tValid Loss:   2.228 | Valid PPL:   9.277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|████████████████████████████████████████████████▊                | 15/20 [1:36:55<45:08, 541.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.267 | Train PPL:   3.551\n",
      "\tValid Loss:   2.212 | Valid PPL:   9.137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████████             | 16/20 [1:47:47<38:19, 574.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.235 | Train PPL:   3.440\n",
      "\tValid Loss:   2.207 | Valid PPL:   9.090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|███████████████████████████████████████████████████████▎         | 17/20 [1:59:10<30:22, 607.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.203 | Train PPL:   3.329\n",
      "\tValid Loss:   2.208 | Valid PPL:   9.095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████▌      | 18/20 [2:10:39<21:04, 632.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.180 | Train PPL:   3.255\n",
      "\tValid Loss:   2.203 | Valid PPL:   9.049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████████████████████████████████████████████████████████▊   | 19/20 [2:23:06<11:06, 666.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.153 | Train PPL:   3.168\n",
      "\tValid Loss:   2.219 | Valid PPL:   9.194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 20/20 [2:35:55<00:00, 467.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   1.132 | Train PPL:   3.102\n",
      "\tValid Loss:   2.216 | Valid PPL:   9.166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "clip = 1.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "date = datetime.today().strftime('%Y-%m-%d')\n",
    "model_dir = f'../models/tatoeba/{date}'\n",
    "# Create the directory\n",
    "try:\n",
    "    os.mkdir(model_dir)\n",
    "    print(f\"Directory '{model_dir}' created successfully.\")\n",
    "except FileExistsError:\n",
    "    print(f\"Directory '{model_dir}' already exists.\")\n",
    "\n",
    "train_lossi = []\n",
    "val_lossi = []  \n",
    "\n",
    "for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "    train_loss = train_fn(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        clip,\n",
    "        teacher_forcing_ratio,\n",
    "        device,\n",
    "    )\n",
    "    valid_loss = evaluate_fn(\n",
    "        model,\n",
    "        valid_data_loader,\n",
    "        criterion,\n",
    "        device,\n",
    "    )\n",
    "\n",
    "    train_lossi.append(train_loss)\n",
    "    val_lossi.append(valid_loss)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), os.path.join(model_dir, f'mt_tatoeba_{train_loss:7.2f}_{valid_loss:7.2f}.pt'))\n",
    "    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {np.exp(valid_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f6d16c-254a-4943-abfa-1f80d580c977",
   "metadata": {},
   "source": [
    "### Evaluating on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9b062474-122f-4ba7-ad31-40c1642d39e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 2.126 | Test PPL:   8.378 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(f'../models/tatoeba/2024-11-30/', f'mt_tatoeba.pt')))\n",
    "\n",
    "test_loss = evaluate_fn(model, test_data_loader, criterion, device)\n",
    "\n",
    "print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3529853-917c-4aeb-821b-8faae4088b46",
   "metadata": {},
   "source": [
    "### BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "489816d4-d19b-4be6-89b2-a20897500f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_nlp,\n",
    "    fr_nlp,\n",
    "    en_vocab,\n",
    "    fr_vocab,\n",
    "    lower,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    "    max_output_length=25,\n",
    "):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if isinstance(sentence, str):\n",
    "            tokens = [token.text for token in fr_nlp.tokenizer(sentence)]\n",
    "        else:\n",
    "            tokens = [token for token in sentence]\n",
    "        if lower:\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "        tokens = [sos_token] + tokens + [eos_token]\n",
    "        ids = fr_vocab.lookup_indices(tokens)\n",
    "        tensor = torch.LongTensor(ids).unsqueeze(-1).to(device)\n",
    "        src_len = [tensor.shape[0]]\n",
    "        hidden, cell = model.encoder(tensor, src_len)\n",
    "        inputs = en_vocab.lookup_indices([sos_token])\n",
    "        for _ in range(max_output_length):\n",
    "            inputs_tensor = torch.LongTensor([inputs[-1]]).to(device)\n",
    "            output, hidden, cell = model.decoder(inputs_tensor, hidden, cell)\n",
    "            predicted_token = output.argmax(-1).item()\n",
    "            inputs.append(predicted_token)\n",
    "            if predicted_token == en_vocab[eos_token]:\n",
    "                break\n",
    "        tokens = en_vocab.lookup_tokens(inputs)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8161c3a8-c0ed-4bbf-b3bf-70eecf88d178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('laissez les mains sur le volant', 'keep your hands on the wheel')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = test_data[0][\"fr\"]\n",
    "expected_translation = test_data[0][\"en\"]\n",
    "\n",
    "sentence, expected_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e338b1c8-758b-4646-ae86-435712b72762",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_nlp,\n",
    "    fr_nlp,\n",
    "    en_vocab,\n",
    "    fr_vocab,\n",
    "    lower,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3494b57c-e64a-4213-aa08-71cad83b61af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>', 'keep', 'your', 'hands', 'on', 'the', 'wheel', '<eos>']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "95016306-2816-4098-9f88-a6cad297b55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Bonjour.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "398efc9f-102f-4337-a919-d8238bc5b25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_nlp,\n",
    "    fr_nlp,\n",
    "    en_vocab,\n",
    "    fr_vocab,\n",
    "    lower,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5df628b6-792f-4c3c-87ad-380b19a75378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>', 'good', 'lesson', '<eos>']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ad9d7364-9992-41a7-8375-93f58eacc62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 13585/13585 [02:56<00:00, 77.07it/s]\n"
     ]
    }
   ],
   "source": [
    "translations = [\n",
    "    translate_sentence(\n",
    "        example[\"fr\"],\n",
    "        model,\n",
    "        en_nlp,\n",
    "        fr_nlp,\n",
    "        en_vocab,\n",
    "        fr_vocab,\n",
    "        lower,\n",
    "        sos_token,\n",
    "        eos_token,\n",
    "        device,\n",
    "    )\n",
    "    for example in tqdm.tqdm(test_dataset)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cc0af41a-ae8c-4fe7-ab78-f6fecbcf8d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "02c9ef00-ebe2-47f5-8acf-e92037144482",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [\" \".join(translation[1:-1]) for translation in translations]\n",
    "\n",
    "references = [[example[\"en\"]] for example in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6fbbabf8-a13b-49a2-9afa-074542db85df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep your hands on the wheel, ['keep your hands on the wheel'], laissez les mains sur le volant\n",
      "his teacher must be filled with him, ['his teacher should be strict with him'], son professeur devrait etre strict avec lui\n",
      "what was you up ?, ['what s kept you ?'], qu est ce qui t a retenu ?\n",
      "i don t know who did it, ['i don t know who did it'], je ne sais pas qui l a fait\n",
      "i make you nervous don t i ?, ['i make you nervous don t i ?'], je te rends nerveux n est ce pas ?\n",
      "i don t have much clue, ['i don t have the slightest clue'], je n en ai pas la moindre idee\n",
      "i don t yet know what i m doing, ['i don t know what i m doing yet'], je ne sais pas encore ce que je fais\n",
      "will you dance with me ?, ['do you want to dance with me ?'], veux tu danser avec moi ?\n",
      "it s not the human size, ['size does not matter'], ce n est pas la taille qui compte\n",
      "i ve heard all about you, ['i ve heard all about you'], j ai tout entendu a ton sujet\n",
      "the early that hotel is too difficult for me, ['the food at that restaurant is too greasy for me'], le menu de ce restaurant est trop gras pour moi\n",
      "he gave us some few drinks, ['he bought us some drinks'], il nous a paye quelques verres\n",
      "she intended intended to go shopping, ['she intended to go shopping'], elle avait l intention d aller faire des emplettes\n",
      "she ripped her children older as they did things things, ['she altered her old clothes to make them look more fashionable'], elle a raccommode ses vieilles fringues pour qu elles aient l air plus a la mode\n",
      "he doesn t speak french but he speaks spanish, ['he doesn t just speak french he speaks spanish as well'], il ne parle pas que francais mais aussi espagnol\n",
      "i have no one to kill me, ['i don t have anybody who ll listen to me'], je n ai personne qui m ecoutera\n",
      "i could ve been kill, ['i could have been killed'], j aurais pu etre tue\n",
      "in far case i m only cold in such small problem, ['then i m afraid we have a bit of a problem'], dans ce cas j ai bien peur qu il y ait un petit probleme\n",
      "why don t you start by telling us who went with us ?, ['why don t you start by telling us who went with you ?'], pourquoi ne commencez vous pas par nous dire qui y est alle avec vous ?\n",
      "tom is a ambitious isn t he ?, ['tom is an architect right ?'], tom est architecte pas vrai ?\n",
      "in a circumstances can can see you, ['in space no one can hear you scream'], dans l espace personne ne peut t entendre crier\n",
      "i heard that the the war was eaten the the winter, ['i hear the grass is green even in the winter in england'], j ai entendu dire qu en angleterre l herbe etait verte meme en hiver\n",
      "it s cheaper to order things things in the dozen, ['it s cheaper to order things by the dozen'], c est moins cher de commander des choses a la douzaine\n",
      "he turned off the edge of his shoulder, ['he turned the table upside down'], il retourna la table sur son plateau\n",
      "who goes playing the this tonight ?, ['who s playing hockey tonight ?'], qui joue au hockey ce soir ?\n",
      "i don t know how to contact you, ['i don t know how to contact you'], je ne sais pas comment prendre contact avec toi\n",
      "the increase of <unk> <unk> numb with, ['the chimney is belching black smoke'], la cheminee crache de la fumee noire\n",
      "i hate that, ['i hate that'], je deteste ca\n",
      "once a glance he must help up, ['once you have made a promise you should keep it'], une fois une promesse faite il faut la respecter\n",
      "when i grow up i d be like you like like you, ['when i grow up i want to be just like you'], quand je serai grande je veux etre exactement comme vous\n",
      "that s all you need to know, ['that s all you need to know'], c est tout ce qu il te faut savoir\n",
      "it would be great if you could join us for dinner, ['it would be great if you could join us for dinner'], ca serait vraiment bien si vous pouviez vous joindre a nous pour dejeuner\n",
      "i tried to warn you, ['i tried to warn you'], j ai essaye de vous prevenir\n",
      "you re ambitious, ['you re ambitious'], tu es ambitieux\n",
      "a sensible man would not like such a thing, ['a wise man would not say such a thing'], un homme sage ne dirait pas une telle chose\n",
      "i don t believe in magic, ['i don t believe in karma'], je ne crois pas au karma\n",
      "i received your letter yesterday, ['i got your letter yesterday'], j ai recu ta lettre hier\n",
      "you re all crazy, ['you re all crazy'], vous etes tous fous\n",
      "i thought you d be taller, ['i thought you d be taller'], je pensais que vous seriez plus grand\n",
      "we walked across the pond, ['we walked around the pond'], nous avons marche autour de l etang\n",
      "the water was dark and shining stars, ['the water was calm and very blue'], l eau etait calme et tres bleue\n",
      "i know that it s highly unlikely that he s ever to help me, ['i know that it is highly unlikely that anyone would be willing to help me'], je sais qu il est hautement improbable que quiconque soit dispose a m aider\n",
      "that is the girl you ve to see, ['this is the girl you wanted to see'], c est la fille que tu voulais voir\n",
      "may i use the phone ?, ['may i use the phone ?'], puis je utiliser le telephone ?\n",
      "is there home home ?, ['anybody home ?'], y a t il quelqu un dans la maison ?\n",
      "you are my hero, ['you are my hero'], tu es mon heros\n",
      "we re wasting water, ['we re wasting water'], nous sommes en train de gaspiller de l eau\n",
      "i love you more than, ['i love you more than anything'], je vous aime plus que tout\n",
      "i ran all the train at the station, ['i ran all the way to the station'], je courus tout le chemin jusqu a la gare\n",
      "he doesn t hold his sense, ['he doesn t show his true feelings'], il ne montre pas ses veritables sentiments\n",
      "i watch television on time time, ['i watch tv now and then'], je regarde la television de temps en temps\n",
      "i m not a only child anymore, ['i m not a baby you know !'], je ne suis plus un bebe !\n",
      "are you ok ?, ['you all right ?'], ca va bien ?\n",
      "the bad <unk> came me my heart, ['the bad smell sickened me'], la mauvaise odeur m a retourne le c ur\n",
      "we have many friends, ['we ve got a lot of friends'], nous avons beaucoup d amis\n",
      "you re not helping me, ['you re not helping me'], tu ne m aides pas\n",
      "he can well well, ['he speaks really well'], il parle vraiment bien\n",
      "he breathed deeply, ['he gave a deep sigh'], il soupira profondement\n",
      "this book is for you, ['this book is for you'], ce livre est pour toi\n",
      "nice nice !, ['have a nice holiday'], bonnes vacances !\n",
      "i live in <unk>, ['i live in hyogo'], je vis a hyogo\n",
      "could you briefly briefly briefly ?, ['could you please briefly introduce yourself ?'], pourrais tu brievement te presenter ?\n",
      "where did you get this ?, ['where did you get it ?'], ou as tu obtenu ceci ?\n",
      "the dog arched my hand, ['the dog bit my hand'], le chien mordit ma main\n",
      "the manager of the <unk> s was <unk>, ['the criticism of the actor s performance was just'], la critique de la representation de l acteur etait juste\n",
      "tom wondered if it was true, ['tom wondered if it was true'], tom s est demande si c etait vrai\n",
      "she will him him again, ['she ll love him forever'], elle l aimera pour toujours\n",
      "this company is frightening about his carelessness, ['this company is indifferent to the safety of its workers'], cette entreprise est indifferente a la securite de ses employes\n",
      "i study french in japanese, ['i study japanese history'], j etudie l histoire japonaise\n",
      "i watched the watch tv before my homework before, ['i watched baseball on tv after i finished my homework'], j ai regarde le baseball a la television apres avoir termine mes devoirs\n",
      "i blame things, ['i ll handle things'], je gererai les choses\n",
      "i m the youngest in the age, ['i m the youngest child'], je suis la plus jeune enfant\n",
      "father extended away after breakfast, ['dad stretched after dinner'], papa s est etendu apres le dejeuner\n",
      "the defendant was guilty guilty of the pain insanity insanity, ['the defendant was found not guilty by reason of insanity'], l accuse fut prononce non coupable en raison de sa folie\n",
      "ants cost him was his his, ['the audience clapped loudly after his speech'], l assistance applaudit bruyamment apres son discours\n",
      "you re not late, ['you re not late'], tu n es pas en retard\n",
      "i felt like a idiot, ['i felt dumb'], je me suis sentie idiote\n",
      "i d like to stay out to stay today today, ['i feel like going out rather than staying at home today'], j aimerais mieux sortir dehors que de rester a la maison aujourd hui\n",
      "this box is big, ['this box is light'], cette caisse est legere\n",
      "do you remember how we met ?, ['do you remember how we met ?'], te rappelles tu comment nous nous sommes rencontrees ?\n",
      "the stock s stock staff staff <unk> signaled its <unk> to a new <unk>, ['the rise in house prices enabled him to sell his house at a big profit'], la hausse des prix de l immobilier lui a permis de vendre sa maison en faisant un gros profit\n",
      "many people are upset, ['many people are upset'], de nombreuses personnes sont fachees\n",
      "we climbed the the hill, ['we ran down the hill'], nous avons descendu la colline en courant\n",
      "they re really <unk>, ['they re really tight'], ils sont vraiment radins\n",
      "you re such a liar !, ['you are such a liar !'], tu es un de ces menteurs !\n",
      "your cat is overweight, ['your cat is overweight'], ton chat est en surpoids\n",
      "the development of the financial easier is greater quicker that it has anything that it it unsaid, ['rather than doing any good the rain did a great deal of harm to the crop'], la pluie a cause de gros degats a la recolte plutot qu elle lui a fait quoi que ce soit de bien\n",
      "could you tell me why you like me ?, ['will you tell me why you like her ?'], pourrais tu me dire pourquoi tu l aimes ?\n",
      "speak speak so please, ['speak louder please'], parlez plus fort s il vous plait\n",
      "you shouldn t be interested in my dog that he does, ['you shouldn t get near my dog while he s eating'], vous ne devriez pas vous approcher de mon chien pendant qu il mange\n",
      "i don t write your opinion, ['i m not writing about you'], je n ecris pas a ton sujet\n",
      "prices went off, ['prices have gone down'], les prix ont baisse\n",
      "i m very impressed, ['i m very poor'], je suis tres pauvre\n",
      "i was up to go, ['i made up my mind to go there'], je me decidai a y aller\n",
      "the prices connects rising billion altitude of meters meters, ['the milky way consists of about a hundred billion stars'], la voie lactee se compose d environ cent milliards d etoiles\n",
      "we re fasting, ['we re taking over'], nous prenons le controle\n",
      "you re smarter than me, ['you re smarter than me'], vous etes plus intelligent que moi\n",
      "why don t you study french ?, ['why aren t you studying french ?'], pourquoi n etudiez vous pas le francais ?\n",
      "we are faced with a era of financial, ['we are faced with a host of problems'], nous sommes confrontes a une foule de problemes\n",
      "it s really nice to hear, ['that s really nice to hear'], c est vraiment agreable a entendre\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(f'{predictions[i]}, {references[i]}, {test_dataset[i][\"fr\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "33c93115-0202-4440-b2fa-ca59d3aa8c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en_ids': tensor([   2,  192,   31,  483,   39,    7, 2844,    3]),\n",
       " 'fr_ids': tensor([   2,  357,   28,  538,   66,   15, 2639,    3]),\n",
       " 'en': 'keep your hands on the wheel',\n",
       " 'fr': 'laissez les mains sur le volant',\n",
       " 'en_tokens': ['<sos>',\n",
       "  'keep',\n",
       "  'your',\n",
       "  'hands',\n",
       "  'on',\n",
       "  'the',\n",
       "  'wheel',\n",
       "  '<eos>'],\n",
       " 'fr_tokens': ['<sos>',\n",
       "  'laissez',\n",
       "  'les',\n",
       "  'mains',\n",
       "  'sur',\n",
       "  'le',\n",
       "  'volant',\n",
       "  '<eos>']}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3f5b4953-269e-4b8e-93bc-f8be471edf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer_fn(nlp, lower):\n",
    "    def tokenizer_fn(s):\n",
    "        tokens = [token.text for token in nlp.tokenizer(s)]\n",
    "        if lower:\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "        return tokens\n",
    "\n",
    "    return tokenizer_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "14f0711f-61b3-4749-88d4-82f81ba9864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_fn = get_tokenizer_fn(en_nlp, lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8160d26c-83ec-4656-93f3-87c2b8d83605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['keep', 'your', 'hands', 'on', 'the', 'wheel'],\n",
       " ['keep', 'your', 'hands', 'on', 'the', 'wheel'])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_fn(predictions[0]), tokenizer_fn(references[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d02c0669-686a-46c4-a9cd-6507e7d33036",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = bleu.compute(\n",
    "    predictions=predictions, references=references, tokenizer=tokenizer_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "60c7cb11-d5b8-4424-b328-e027b4c80365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.4440880208080938,\n",
       " 'precisions': [0.6884195643529359,\n",
       "  0.49601673879423497,\n",
       "  0.3825952463621729,\n",
       "  0.30454399614983557],\n",
       " 'brevity_penalty': 0.994338536351338,\n",
       " 'length_ratio': 0.9943545021198076,\n",
       " 'translation_length': 90532,\n",
       " 'reference_length': 91046}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5bebb8-1e17-4221-9d55-7f3c4245fe00",
   "metadata": {},
   "source": [
    "### Cross testing with Wiki test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "594e9f3a-8e39-416a-ad9d-5ea81c928276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 2.126 | Test PPL:   8.378 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(f'../models/tatoeba/2024-11-30/', f'mt_tatoeba.pt')))\n",
    "\n",
    "test_loss = evaluate_fn(model, test_data_loader, criterion, device)\n",
    "\n",
    "print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f37317b4-1c8c-41f6-add7-11f6d64402b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "\n",
    "def tmx_to_tabbed_txt(tmx_file, output_file):\n",
    "    \"\"\"Extracts English and Hawaiian translations from a TMX file and saves them as tab-separated pairs.\"\"\"\n",
    "    \n",
    "    # Define the XML namespace for `xml:lang`\n",
    "    namespaces = {'xml': 'http://www.w3.org/XML/1998/namespace'}\n",
    "    \n",
    "    # Parse the TMX file\n",
    "    xml_tree = etree.parse(tmx_file)\n",
    "    trans_units = xml_tree.findall(\".//tu\")\n",
    "\n",
    "    pairs = []\n",
    "    \n",
    "    # Open the output file for writing\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as out_file:\n",
    "        # Iterate over each translation unit\n",
    "        for trans_unit in trans_units:\n",
    "            pair = []\n",
    "            source_text = trans_unit.find(\".//tuv[@xml:lang='en']/seg\", namespaces)\n",
    "            target_text = trans_unit.find(\".//tuv[@xml:lang='fr']/seg\", namespaces)\n",
    "\n",
    "            # Write the tab-separated pair if both texts are available\n",
    "            if source_text is not None and target_text is not None and source_text.text and target_text.text:\n",
    "                out_file.write(f\"{source_text.text}\\t{target_text.text}\\n\")\n",
    "                pair.append(source_text.text)\n",
    "                pair.append(target_text.text)\n",
    "                pairs.append(pair)\n",
    "\n",
    "    return pairs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tmx_file = \"../data/wiki/en-fr.tmx\"\n",
    "    output_file = \"../data/wiki/en-fr.txt\"\n",
    "    wiki_pairs = tmx_to_tabbed_txt(tmx_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f9b9dd68-6ec3-4f3e-8b7e-5fdc9e5c0a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1365840"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_pairs = [[normalizeString(sentence) for sentence in pair] for pair in wiki_pairs]\n",
    "len(wiki_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2a1e0dff-86e2-4bd5-aaad-f5c9fb1a24dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137932"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_cleaned_pairs = [pair for pair in wiki_pairs if (len(pair[0]) > 0 and len(pair[0]) <= 41) and (len(pair[0]) > 0 and len(pair[0]) <= 41)]\n",
    "len(wiki_cleaned_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8dc20232-35b6-4b69-acf2-e5b261ad5794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'en': 'third stream', 'fr': 'le third stream'},\n",
       " {'en': 'gil evans influence', 'fr': 'l influence de gil evans'},\n",
       " {'en': 'the horn in the spotlight', 'fr': 'le cor sous les projecteurs'},\n",
       " {'en': 'contemporary horn in jazz', 'fr': 'le cor dans le jazz contemporain'},\n",
       " {'en': 'horn in jazz', 'fr': 'utilisateur djiboun le cor dans le jazz'},\n",
       " {'en': 'murder of jewish civil rights activists',\n",
       "  'fr': 'meurtre de militants juifs des droits civiques'},\n",
       " {'en': 'questioning the golden age', 'fr': 'interrogations sur l age d or'},\n",
       " {'en': 'the problem was the condescending tone',\n",
       "  'fr': 'le probleme etait le ton condescendant'},\n",
       " {'en': 'press of mississippi p lindemann albert s',\n",
       "  'fr': 'black jewish relations on trial leo frank and jim conley in the new south univ'},\n",
       " {'en': 'edwards brent hayes', 'fr': 'note on the text'}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_data = [{'en': pair[0], 'fr': pair[1]} for pair in wiki_cleaned_pairs]\n",
    "wiki_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "06540209-926c-4809-99bc-b72d857c3595",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "random.shuffle(wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "302f7eef-6eb0-4fed-8d9e-18d49d60f210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splittng data\n",
    "wiki_n = len(wiki_data)\n",
    "wiki_n_train = int(0.8*wiki_n)\n",
    "wiki_n_val = int(0.9*wiki_n)\n",
    "\n",
    "wiki_train_data = wiki_data[:wiki_n_train]\n",
    "wiki_val_data = wiki_data[wiki_n_train:wiki_n_val]\n",
    "wiki_test_data = wiki_data[wiki_n_val:]\n",
    "\n",
    "# convert to Huggingface dataset\n",
    "wiki_train_dataset = Dataset.from_list(wiki_train_data)\n",
    "wiki_val_dataset = Dataset.from_list(wiki_val_data)\n",
    "wiki_test_dataset = Dataset.from_list(wiki_test_data)\n",
    "\n",
    "# Create the DatasetDict\n",
    "wiki_dataset_dict = DatasetDict({\n",
    "    \"train\": wiki_train_dataset,\n",
    "    \"validation\": wiki_val_dataset,\n",
    "    \"test\": wiki_test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "12828b69-7245-441e-b296-a3bee2d4393d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'fr'],\n",
       "        num_rows: 110345\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['en', 'fr'],\n",
       "        num_rows: 13793\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'fr'],\n",
       "        num_rows: 13794\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b72cfeb1-5c48-4a51-aaf5-489f9870a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "fr_nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "42a0a1f8-3d18-44e8-8bd6-9058136c34e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', 'a', 'lovely', 'day', 'it', 'is', 'today', '!']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"What a lovely day it is today!\"\n",
    "\n",
    "[token.text for token in en_nlp.tokenizer(string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a43c1d4c-1b61-4b47-9787-59ab99236d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_example(example, en_nlp, fr_nlp, max_length, lower, sos_token, eos_token):\n",
    "    en_tokens = [token.text for token in en_nlp.tokenizer(example[\"en\"])][:max_length]\n",
    "    fr_tokens = [token.text for token in fr_nlp.tokenizer(example[\"fr\"])][:max_length]\n",
    "    if lower:\n",
    "        en_tokens = [token.lower() for token in en_tokens]\n",
    "        fr_tokens = [token.lower() for token in fr_tokens]\n",
    "    en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "    fr_tokens = [sos_token] + fr_tokens + [eos_token]\n",
    "    return {\"en_tokens\": en_tokens, \"fr_tokens\": fr_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d8fc62a8-e1c0-4f0e-9e32-23c47ee2a51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a16ce6af3042bf8a3d051d2e698501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/110345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdae9022e3084a2abeaee5525b6889e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13793 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0187b3f86404cd6a74346d7052016bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13794 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = 40\n",
    "lower = True\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "fn_kwargs = {\n",
    "    \"en_nlp\": en_nlp,\n",
    "    \"fr_nlp\": fr_nlp,\n",
    "    \"max_length\": max_length,\n",
    "    \"lower\": lower,\n",
    "    \"sos_token\": sos_token,\n",
    "    \"eos_token\": eos_token,\n",
    "}\n",
    "\n",
    "wiki_train_dataset = wiki_train_dataset.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "wiki_val_dataset = wiki_val_dataset.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "wiki_test_dataset = wiki_test_dataset.map(tokenize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5a6f35a9-c0e2-4506-8ce6-2f732f1340f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "wiki_min_freq = 2\n",
    "wiki_unk_token = \"<unk>\"\n",
    "wiki_pad_token = \"<pad>\"\n",
    "\n",
    "special_tokens = [\n",
    "    wiki_unk_token,\n",
    "    wiki_pad_token,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "]\n",
    "\n",
    "wiki_en_vocab = build_vocab_from_iterator(\n",
    "    wiki_train_dataset[\"en_tokens\"],\n",
    "    min_freq=wiki_min_freq,\n",
    "    specials=special_tokens,\n",
    ")\n",
    "\n",
    "wiki_fr_vocab = build_vocab_from_iterator(\n",
    "    wiki_train_dataset[\"fr_tokens\"],\n",
    "    min_freq=wiki_min_freq,\n",
    "    specials=special_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ec7b75e7-33a2-4d78-90b9-0225355af56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert wiki_en_vocab[wiki_unk_token] == wiki_fr_vocab[wiki_unk_token]\n",
    "assert wiki_en_vocab[wiki_pad_token] == wiki_fr_vocab[wiki_pad_token]\n",
    "\n",
    "wiki_unk_index = wiki_en_vocab[wiki_unk_token]\n",
    "wiki_pad_index = wiki_en_vocab[wiki_pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "39bb02dc-6d63-406b-95d5-193a7193e94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_en_vocab.set_default_index(wiki_unk_index)\n",
    "wiki_fr_vocab.set_default_index(wiki_unk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a7777846-da11-48b7-9f13-4b754a0a69ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_example(example, en_vocab, fr_vocab):\n",
    "    en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])\n",
    "    fr_ids = fr_vocab.lookup_indices(example[\"fr_tokens\"])\n",
    "    return {\"en_ids\": en_ids, \"fr_ids\": fr_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dc76bc6f-ac92-4a7c-a7dd-fbb1c4f7ede1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea6f67cf78b46c29502fe43c6311533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/110345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac2d0796fcd4118aa8d02371933dfc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13793 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a43aaa2e1e5443fbb0f23dcbdc69cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13794 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fn_kwargs = {\"en_vocab\": wiki_en_vocab, \"fr_vocab\": wiki_fr_vocab}\n",
    "\n",
    "wiki_train_dataset = wiki_train_dataset.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "wiki_val_dataset = wiki_val_dataset.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "wiki_test_dataset = wiki_test_dataset.map(numericalize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dd530b-1265-4722-b3be-c1cb51b5a9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_type = \"torch\"\n",
    "# format_columns = [\"en_ids\", \"fr_ids\"]\n",
    "\n",
    "# wiki_train_dataset = wiki_train_dataset.with_format(\n",
    "#     type=data_type, columns=format_columns, output_all_columns=True\n",
    "# )\n",
    "\n",
    "# wiki_val_dataset = wiki_val_dataset.with_format(\n",
    "#     type=data_type,\n",
    "#     columns=format_columns,\n",
    "#     output_all_columns=True,\n",
    "# )\n",
    "\n",
    "# wiki_test_dataset = wiki_test_dataset.with_format(\n",
    "#     type=data_type,\n",
    "#     columns=format_columns,\n",
    "#     output_all_columns=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beba826a-25ed-4aae-8c45-271aa197aeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_batch_size = 128\n",
    "\n",
    "# wiki_train_data_loader = get_data_loader(wiki_train_dataset, wiki_batch_size, wiki_pad_index, shuffle=True)\n",
    "# wiki_valid_data_loader = get_data_loader(wiki_val_dataset, wiki_batch_size, wiki_pad_index)\n",
    "# wiki_test_data_loader = get_data_loader(wiki_test_dataset, wiki_batch_size, wiki_pad_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ad2bd-eb91-4857-82e7-182d4c318db9",
   "metadata": {},
   "source": [
    "### BLEU score on wiki test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "06fc829b-1094-4d99-8552-c307772a22ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 13794/13794 [03:12<00:00, 71.70it/s]\n"
     ]
    }
   ],
   "source": [
    "translations = [\n",
    "    translate_sentence(\n",
    "        example[\"fr\"],\n",
    "        model,\n",
    "        en_nlp,\n",
    "        fr_nlp,\n",
    "        en_vocab,\n",
    "        fr_vocab,\n",
    "        lower,\n",
    "        sos_token,\n",
    "        eos_token,\n",
    "        device,\n",
    "    )\n",
    "    for example in tqdm.tqdm(wiki_test_dataset)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f95f743b-1e26-4754-9676-a155183be2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "deaaf313-167a-440f-8ef9-2c4f04b48ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [\" \".join(translation[1:-1]) for translation in translations]\n",
    "\n",
    "references = [[example[\"en\"]] for example in wiki_test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b938693c-0287-4211-9876-9826d8b67d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let s <unk>, ['national songs']\n",
      "<unk> the <unk> <unk> <unk> <unk> the <unk> destruction of the club, ['retrieved']\n",
      "the heavy tunnel of <unk> <unk> the <unk>, ['cartoon influence beginnings of posing']\n",
      "let s negotiate, ['res lett']\n",
      "darwin <unk>, ['subfeatures']\n",
      "dig the deep steep flag, ['doge s palace built']\n",
      "ask a lot, ['and not much !']\n",
      "<unk> <unk> <unk> and <unk> <unk>, ['grants fellowships literary awards prizes']\n",
      "despite his sheep were <unk>, ['taiwan and its surrounding islands']\n",
      "<unk> <unk> <unk> to <unk>, ['morison was born in hammersmith in']\n",
      "he has no children to for the s strength, ['there were no children of either marriage']\n",
      "the smells <unk>, ['ancient history']\n",
      "they children kids, ['they had the following children']\n",
      "<unk> <unk> in the tenth, ['christy turlington']\n",
      "<unk> <unk> <unk>, ['sifakis stewart']\n",
      "your english and moves and, ['public health and prevention strategies']\n",
      "sleep, ['effects of dyes pigments']\n",
      "i m interfering, ['coach manager']\n",
      "some choices are, ['aba commission of enquiry']\n",
      "start with letters with the letters, ['title release date publisher notes ref']\n",
      "edison <unk> <unk> <unk> <unk>, ['milking parlour dairy farming wastes']\n",
      "let s negotiate, ['it consisted of']\n",
      "defend the remote remote, ['calihan hall records']\n",
      "let s <unk>, ['junior officers']\n",
      "let s negotiate, ['reminiscences']\n",
      "<unk> <unk> tribes <unk> threatened, ['historical population census pop est']\n",
      "we were in all all week, ['we were in the studio all this week']\n",
      "come, ['personhood']\n",
      "this vending package was bombed in the steep n, ['december human rights watch report']\n",
      "let s <unk>, ['capitalnewyork com']\n",
      "<unk> <unk> arose into <unk>, ['hewa bora airways flight']\n",
      "in <unk> soared, ['rugby union']\n",
      "<unk> <unk> <unk> <unk>, ['isbn green stanley march']\n",
      "he s unconscious, ['he died in']\n",
      "let s act, ['final glory']\n",
      "<unk> <unk> <unk> drained, ['isbn isbn barber j d']\n",
      "he was opposed to <unk>, ['in she was reduced to a gun ship']\n",
      "there is thirty equal, ['california state university']\n",
      "charity percent shot, ['self care requisites']\n",
      "first <unk> <unk> <unk> up <unk> <unk>, ['cypriot first division cypriot super cup']\n",
      "<unk> <unk> <unk> unharmed, ['introduction to modern set theory']\n",
      "let s <unk>, ['kevin healey autism activist']\n",
      "let s <unk>, ['ships hit by u']\n",
      "the leaves turns rising, ['tsutenkaku south side']\n",
      "<unk> <unk> <unk>, ['contemporary diplomatic passports']\n",
      "divide <unk>, ['westhafen canal']\n",
      "part of the <unk> <unk> is is pointless, ['animal personhood']\n",
      "turn it up, ['accessed november']\n",
      "the streets were were <unk>, ['the eight members were']\n",
      "let s <unk>, ['lifetime oza']\n",
      "i don t even either more consequences, ['nor do i know russian hackers']\n",
      "our yacht club is growing, ['virginia house of delegates']\n",
      "he writes a a <unk> attitude your fate, ['he wrote one hymn lift up your hearts !']\n",
      "space international <unk> <unk> <unk> <unk> in the <unk> in the <unk> <unk> deficit deficit deficit deficit will be <unk> to the <unk> to, ['chumlee russell the history channel']\n",
      "only <unk> <unk> <unk> <unk>, ['king of dai zhao jia']\n",
      "<unk> is <unk> to the end, ['martinsburg july']\n",
      "let s <unk>, ['convoys escorted']\n",
      "twenty twenty few doctor, ['education and career']\n",
      "<unk> <unk> <unk>, ['further structural imaging evidence']\n",
      "every lesson is working, ['each game costs']\n",
      "he he to the in the, ['in he defended the breisgau']\n",
      "<unk> quickly up <unk>, ['signing to lab records']\n",
      "the building building was covered in <unk>, ['destroyed building was restored in']\n",
      "there is fifty, ['erdan island']\n",
      "go in the market, ['retrieved july']\n",
      "<unk> <unk> <unk>, ['black lives matter']\n",
      "let s <unk>, ['energy resources mines']\n",
      "he s a controversial talker, ['it is a planar molecule']\n",
      "let s negotiate, ['wega horse']\n",
      "barack <unk> is growing, ['dof laboratory']\n",
      "<unk> <unk> arose into <unk> <unk>, ['david william murray rd earl of mansfield']\n",
      "let s negotiate, ['isbn']\n",
      "mary was born in the, ['maria was born in evora on april']\n",
      "<unk> is born in the <unk>, ['webb was born on july in monmouth']\n",
      "for a flame, ['military service']\n",
      "<unk> <unk> <unk> into a circle, ['derek barbolla founded cercle in']\n",
      "his neighbors are, ['her instructions were']\n",
      "i m not not, ['historically i am nonpartisan']\n",
      "he is buried in the city of town, ['he is buried in the same city']\n",
      "rumors of the loans flew to the steep tower, ['modern view']\n",
      "get it, ['season']\n",
      "let s negotiate, ['retrieved january']\n",
      "his <unk> <unk> <unk> <unk>, ['her main mast is metres ft in']\n",
      "divide a parts, ['rugby union career']\n",
      "cool is, ['chemical sciences']\n",
      "good <unk> has issued the nobel, ['ideation in corporate governance']\n",
      "let s be, ['boys singles']\n",
      "<unk> s leaves billowed <unk>, ['har built rescue']\n",
      "let s negotiate, ['invasive species']\n",
      "let s negotiate, ['contributor']\n",
      "<unk> <unk> up in the days, ['aged matured for at least days']\n",
      "let s <unk>, ['volans kevin n d contradictory']\n",
      "let s showing the new tv, ['tisha at television film']\n",
      "let s establish <unk> <unk>, ['sound installation by dipna horra']\n",
      "<unk> <unk> <unk> to <unk> <unk>, ['nakalembe grew up in kampala uganda']\n",
      "despite the <unk> <unk> the magnitude bearing the spine, ['mithrakanth of india won with']\n",
      "almost war war and stopped, ['world war ii and later']\n",
      "the <unk> <unk> <unk> soared, ['the main dancer of the evil boys']\n",
      "what is <unk> ?, ['what is queeruption ?']\n",
      "clean your tv, ['tv series']\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(f'{predictions[i]}, {references[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "322f6114-b855-4750-872a-fb5e69ab8b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'national songs',\n",
       " 'fr': 'musiques nationales',\n",
       " 'en_tokens': ['<sos>', 'national', 'songs', '<eos>'],\n",
       " 'fr_tokens': ['<sos>', 'musiques', 'nationales', '<eos>'],\n",
       " 'en_ids': [2, 111, 593, 3],\n",
       " 'fr_ids': [2, 5012, 2367, 3]}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "02a027bf-32b1-417a-9d97-127b3684bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer_fn(nlp, lower):\n",
    "    def tokenizer_fn(s):\n",
    "        tokens = [token.text for token in nlp.tokenizer(s)]\n",
    "        if lower:\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "        return tokens\n",
    "\n",
    "    return tokenizer_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e0db85a9-d262-4274-be9c-389edf2ecbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_fn = get_tokenizer_fn(en_nlp, lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c5bbabbd-0b93-436c-89f5-0976fd2f38b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['let', 's', '<', 'unk', '>'], ['national', 'songs'])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_fn(predictions[0]), tokenizer_fn(references[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f81dbfd1-9960-430c-8b4e-941777162906",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = bleu.compute(\n",
    "    predictions=predictions, references=references, tokenizer=tokenizer_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "053c8343-0671-4d66-966f-c4b715d9c05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.011042315644887286,\n",
       " 'precisions': [0.08006872237235356,\n",
       "  0.01810943242139649,\n",
       "  0.005576583097020681,\n",
       "  0.0018386739090768733],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.9755343032402906,\n",
       " 'translation_length': 111754,\n",
       " 'reference_length': 56569}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312995c2-e0ed-4f6e-8c98-fbdd77d31fda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
